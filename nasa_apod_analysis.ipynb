{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NASA APOD Data Retrieval and Analysis Project\n",
    "\n",
    "## The steps I will do in this section:\n",
    "1. Getting API Key\n",
    "2. Connecting to NASA APOD API\n",
    "3. The Accumulation, Storage, and Processing of Data\n",
    "4. The Interpretation and Visualisation of Data\n"
   ],
   "id": "91043e015d4bb172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:49:08.991514Z",
     "start_time": "2024-11-19T15:49:08.986161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# dowloand .env file \n",
    "load_dotenv()\n",
    "\n",
    "# get api key\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"API key is not found\")\n",
    "else:\n",
    "    print(\"API key successfully retrieved\")\n"
   ],
   "id": "2a1b1000d23bda4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key successfully retrieved\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem 1: NASA APOD Data Retrieval and JSON File Processing (33 marks)\n",
    "**I can set the API key by running the code below, but it did not work on my computer. I solved it by using the `python-dotenv` library instead.**\n",
    "\n",
    "- **Windows:** `setx API_KEY \"your_api_key\"`\n",
    "- **MacOS/Linux:** `export API_KEY=\"your_api_key\"`\n"
   ],
   "id": "c2ad1980d213355e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1 API CALL - `get_apod_data` Function (10 marks)\n",
    "\n",
    "#### Purpose?\n",
    "- Pull visual information for a specific date using NASA's APOD (Astronomy Picture of the Day) API.\n",
    "\n",
    "#### Definitions of Functions Used:\n",
    "\n",
    "- **Function Name**: `get_apod_data`\n",
    "- **Parameters**:\n",
    "  - `api_key` (str): NASA APOD API key\n",
    "  - `date` (str): The date we want to pull the data from is specified in the documentation as Format: `\"YYYY-MM-DD\"`\n",
    "    \n",
    "- **Return**: Data retrieved from the API will be returned as a dictionary\n",
    "  - `date`: The date of the data\n",
    "  - `title`: The title of the data\n",
    "  - `media_type`: The type of media (image or video)\n",
    "  - `url`: URL of the media\n",
    "  - `explanation`: Explanation of the media\n",
    "\n",
    "#### Error Handling:\n",
    "- The function is designed using the `try-except` block, so that it handles errors that may occur when connecting to the API and displays the error message caught to the user.\n",
    "\n",
    "\n"
   ],
   "id": "aa889d084e7c05cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def get_apod_data(api_key, date):\n",
    "    \"\"\"\n",
    "     Retrieves data by sending a request for a specific date\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): NASA APOD API key\n",
    "    - date (str): The date we want to pull the data from is specified in the documentation as Format: \"YYYY-MM-DD\"\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the data (date, title, media type, URL, description)\n",
    "    - None: If the request fails user will be informed\n",
    "    \"\"\"\n",
    "    url = \"https://api.nasa.gov/planetary/apod\"  # API endpoint\n",
    "    params = {\n",
    "        'api_key': api_key,  # Add the API key to the parameters\n",
    "        'date': date  # Add the date to the parameters\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the API\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Throws HTTPError if there is an error\n",
    "        data = response.json()  # Retrieves the response in JSON format as a dictionary\n",
    "\n",
    "        # Return the data as a dictionary\n",
    "        return {\n",
    "            'date': data.get('date'),\n",
    "            'title': data.get('title'),\n",
    "            'media_type': data.get('media_type'),\n",
    "            'url': data.get('url'),\n",
    "            'explanation': data.get('explanation')\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # In case of error, the user is informed\n",
    "        print(f\"An error occurred during the API request: {e}\")\n",
    "        return None\n"
   ],
   "id": "8ec1dfc73b825cf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = get_apod_data(api_key, \"1999-11-01\")  # My birthday :)))\n",
    "print(result)"
   ],
   "id": "ee22f6adfe1895b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2 Fetching Data for a Date Range (15 marks) - `fetch_multiple_apod_data` Function\n",
    "\n",
    "#### Purpose?\n",
    "\n",
    "This function will pull NASA APOD data for each day within a **specific date range** (e.g. one year). The captured data will be saved in a JSON file for later analysis. This step involves the process of extracting and saving bulk data.\n",
    "\n",
    "#### Definitions of Functions Used:\n",
    "\n",
    "- **Func Name**: `fetch_multiple_apod_data`\n",
    "- **Parametres**:\n",
    "  - `api_key` (str): API KEY\n",
    "  - `start_date` (str): Start date. Format: `\"YYYY-MM-DD\"`.\n",
    "  - `end_date` (str): End date. Format: `\"YYYY-MM-DD\"`.\n",
    "- **İşleyiş**:\n",
    "  - calls the `get_apod_data` function daily for the specified date period.\n",
    "  - A **1 second delay** is added after each request to maintain the API rate limit.\n",
    "  - The data is written to a JSON file in **append mode**.\n",
    "\n",
    "#### Error Handling:\n",
    "\n",
    "- Network errors that may occur when making requests within the date range are checked.\n",
    "- Any errors that may occur during file writing (for example, permission errors) are checked and a meaningful message is displayed to the user."
   ],
   "id": "7a73de14c1999126"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_multiple_apod_data(api_key, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Pulls NASA APOD data for each day in the specified date range and saves it to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): NASA APOD API key\n",
    "    - start_date (str): Start date (in the format \"YYYY-MM-DD\")\n",
    "    - end_date (str): End date (in the format \"YYYY-MM-DD\")\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Dosya adı\n",
    "    file_name = \"apod_data.json\"\n",
    "\n",
    "    # I check here if the file exists or not with try and except block\n",
    "    try:\n",
    "        with open(f\"data/{file_name}\", \"a\") as file:\n",
    "            while current_date <= end_date:\n",
    "                # We format the date as string because the API receives the date in this format\n",
    "                date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # Get data from the API\n",
    "                data = get_apod_data(api_key, date_str)\n",
    "\n",
    "                if data:\n",
    "                    # I add the data to the file in JSON format\n",
    "                    json.dump(data, file)\n",
    "                    file.write(\"\\n\")  # Adds new line after each entry\n",
    "                    print(f\"Data added for {date_str}\")\n",
    "\n",
    "                # Moving to the next day because we are traversing the date range\n",
    "                current_date += timedelta(days=1)\n",
    "                # Adding  1second delay to comply with the API rate limit\n",
    "                time.sleep(1)\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n"
   ],
   "id": "9f49836b8a2aae51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Explanation\n",
    "\n",
    "This function saves the data to the JSON file by calling the `get_apod_data` function for each day in the given date range. We comply with the API rate limit by adding  1-second delay to each request during processing. \n",
    "\n",
    "- **File Operation**: If the `apod_data.json` file does not exist, it will be created as a new file. Each piece of data is added and a new line is passed for the next entry.\n"
   ],
   "id": "b90b40a61c6bfb97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test etme\n",
    "fetch_multiple_apod_data(api_key, \"2020-01-01\", \"2020-01-25\")"
   ],
   "id": "b8c5c22d9bfaa2c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3 Saving Data to a JSON File (8 marks) - `read_apod_data` Function\n",
    "\n",
    "#### Purpose?\n",
    "Now, we will edit the `fetch_multiple_apod_data` function and add the captured data to a file named `apod_data.json`. If the file does not exist, the function will create the file and save the new data.\n",
    "\n",
    "\n",
    "#### Required Characteristics of the Position:\n",
    "\n",
    "- If the file already exists, we must add the new data without overwriting it.\n",
    "- We must address errors that may occur during file operations (for example, file permission errors).\n",
    "- To increase the size of the JSON file, ensure that at least 200-300 days of data is extracted.\n",
    "- FYI: 200-300 data cannot be withdrawn daily. Reading the documentation the daily limit is not more than 35. This was seen by testing in Postman.\n",
    "\n",
    "#### Definitions of Functions Used:\n",
    "\n",
    "- I checked whether the file exists and if it did, I opened it in append mode, otherwise I opened it in write mode.\n",
    "- Pull data from API for each day and add it to the file.\n",
    "- Implemented error management during file writing."
   ],
   "id": "f2e1da959ffe714a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def fetch_multiple_apod_data(api_key, start_date, end_date, file_name=\"apod_data.json\"):\n",
    "    \"\"\"\n",
    "    Pulls NASA APOD data for each day in the specified date range and saves it to a JSON file. If the file does not exist, it creates it and handles errors that may occur during file writing\n",
    "    \n",
    "    Args:\n",
    "    - api_key (str): NASA APOD API key\n",
    "    - start_date (str): Start date (in the format \"YYYY-MM-DD\")\n",
    "    - end_date (str): End date (in the format \"YYYY-MM-DD\")\n",
    "    - file_name (str): The name of the JSON file. Defaults to \"apod_data.json\".\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    try:\n",
    "        # If the file exists, open it in append mode; otherwise, open it in create and write mode.\n",
    "        with open(f\"data/{file_name}\", \"a\" if os.path.exists(f\"data/{file_name}\") else \"w\") as file:\n",
    "            while current_date <= end_date:\n",
    "                date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "                # Get data from the API\n",
    "                data = get_apod_data(api_key, date_str)\n",
    "\n",
    "                if data:\n",
    "                    # Add the data to the file in JSON format\n",
    "                    json.dump(data, file)\n",
    "                    file.write(\"\\n\")  # Add a new line after each entry\n",
    "                    print(f\"Data added for {date_str}.\")\n",
    "                else:\n",
    "                    print(f\"No data for {date_str}.\")\n",
    "\n",
    "                # Moving to the next day because we are traversing the date range\n",
    "                current_date += timedelta(days=1)\n",
    "\n",
    "                # Delay\n",
    "                time.sleep(1)\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"Something wrong check: {e}\")"
   ],
   "id": "5624d2aebaa3918b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The explanation\n",
    "\n",
    "* * If os.path.exists(file_name): \"a\" otherwise \"w\" Detects file existence and adds it (`\"a\"`) or writes it (`\"w\"`).\n",
    "\n",
    "- Error Management: Detects `IOError` and other file errors using `try-except` block and shows a notice.\n",
    "\n",
    "**Insert New Line**: Use file.write(\"n\") to insert a new line after each JSON record.\n",
    "\n"
   ],
   "id": "fd355a42771228b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fetch_multiple_apod_data(api_key, \"2020-01-26\", \"2020-01-30\")",
   "id": "d23d84075ed599f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem 2: JSON Data Reading, Looping, and Processing (27 Marks)\n",
    "## 2.1 Reading and Loading JSON Data with Exception Handling (10 marks) - `read_apod_data` Function\n",
    "#### Purpose\n",
    "Here, we will load the data into a usable list structure in Python by reading the `apod_data.json` file we created before. I will also have addressed possible errors during file reading.\n",
    "\n",
    "\n",
    "#### Definitions of Functions Used:\n",
    "\n",
    "- **Function Name**: `read_apod_data`\n",
    "- **Operation**:\n",
    " - Checks whether the file exists and is readable\n",
    "   - Reads the JSON file line by line and loads each line as a JSON object\n",
    "     - Handles error conditions (`FileNotFoundError`, `PermissionError`, `JSONDecodeError`) and returns meaningful messages to the user.\n"
   ],
   "id": "daa3071942c0e8a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def read_apod_data(file_name=\"apod_data.json\"):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and loads the data into a list.\n",
    "    \n",
    "    Args:\n",
    "    - file_name (str): The name of the JSON file to read. Defaults to \"apod_data.json\".\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list containing the JSON data from the file.\n",
    "    - None: If an error occurs, it returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(f\"data/{file_name}\", \"r\") as file:\n",
    "            # Load the data from the file\n",
    "            data = [json.loads(line) for line in file]\n",
    "            print(f\"{file_name} file loaded successfully.\")\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_name} file not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"No permission to read {file_name}.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from {file_name}.\")\n",
    "    return None\n"
   ],
   "id": "d2441d4f49709523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "apod_data = read_apod_data()\n",
    "if apod_data:\n",
    "    print(\"Total number of data:\", len(apod_data))\n",
    "    print(\"First Data:\", apod_data[0])"
   ],
   "id": "33cae419156e6de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Processing and Summarizing Data Using Loops (10 marks) - `analyze_apod_media` Function\n",
    "\n",
    "#### Purpose?_\n",
    "\n",
    "If this is the task, these are the things to do ⤵️\n",
    "- Counts the total number of **images** and **video** in the JSON file.\n",
    "- Finds the date with the longest description.\n",
    "\n",
    "#### Definitions of Functions Used:\n",
    "\n",
    "- **Func Name**: `analyze_apod_media`\n",
    "- **How is work**:\n",
    " - Counts the media types (image and video) of the data\n",
    "   - Finds the date with the longest description and measures the length of this description and presents it to the user\n",
    "\n"
   ],
   "id": "12ccbc1cb2205bd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyze_apod_media(file_name=\"apod_data.json\"):\n",
    "    \"\"\"\n",
    "    It checks the JSON file for the number of photos and videos and the longest description date.\n",
    "\n",
    "    Args:\n",
    "    - file_name (str): The name of the JSON file to read. Defaults to \"apod_data.json\".\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing the results of the analysis.\n",
    "    \"\"\"\n",
    "    data = read_apod_data(file_name)\n",
    "    if not data:\n",
    "        return \"Data Can't find! \"\n",
    "\n",
    "    image_count = 0\n",
    "    video_count = 0\n",
    "    longest_explanation_date = \"\"\n",
    "    max_explanation_length = 0\n",
    "\n",
    "    for entry in data:\n",
    "        media_type = entry.get('media_type', '')\n",
    "        explanation = entry.get('explanation', '')\n",
    "\n",
    "        # Medya türüne göre sayım yap\n",
    "        if media_type == 'image':\n",
    "            image_count += 1\n",
    "        elif media_type == 'video':\n",
    "            video_count += 1\n",
    "\n",
    "        # En uzun açıklamayı bul\n",
    "        if len(explanation) > max_explanation_length:\n",
    "            max_explanation_length = len(explanation)\n",
    "            longest_explanation_date = entry.get('date', '')\n",
    "\n",
    "    return {\n",
    "        \"image_count\": image_count,\n",
    "        \"video_count\": video_count,\n",
    "        \"longest_explanation_date\": longest_explanation_date,\n",
    "        \"max_explanation_length\": max_explanation_length\n",
    "    }\n"
   ],
   "id": "8ccf5e1595c093d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = analyze_apod_media()\n",
    "if result:\n",
    "    print(\"Total image count:\", result[\"image_count\"])\n",
    "    print(\"Total video count:\", result[\"video_count\"])\n",
    "    print(\"Date of the longest description:\", result[\"longest_explanation_date\"])\n",
    "    print(\"Length of the longest description:\", result[\"max_explanation_length\"])"
   ],
   "id": "5d1a9d67c418f0a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Extracting and Writing Data to a CSV File (7 marks) - `write_apod_summary_to_csv` Function\n",
    "\n",
    "#### Purpose?\n",
    "\n",
    "- This function will read the data from the JSON file and write it to a CSV file containing certain fields (`date`, `title`, `media_type`, `url`)\n",
    "\n",
    "#### Definitions:\n",
    "\n",
    "- **Function name**: `write_apod_summary_to_csv`\n",
    "- **What it does?**:\n",
    "  - Reads JSON data and writes specified fields (date, title, media_type, url) to CSV file.\n",
    "    - The new data is appended to an existing CSV file.\n",
    "        - Maintains record format and handles file I/O faults.\n",
    "\n"
   ],
   "id": "bd13a7ebdbf6a221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv  # !pip install csv\n",
    "\n",
    "\n",
    "def write_apod_summary_to_csv(json_file=\"apod_data.json\", csv_file=\"apod_summary.csv\"):\n",
    "    \"\"\"\n",
    "   It reads data from a JSON file and writes it to a CSV file containing specific fields.\n",
    "    \n",
    "    Args:\n",
    "    - json_file (str): The name of the JSON file to read. Defaults to \"apod_data.json\".\n",
    "    - csv_file (str): The name of the CSV file to write. Defaults to \"apod_summary.csv\".\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    data = read_apod_data(json_file)\n",
    "    if not data:\n",
    "        return \"No data available to write to CSV.\"\n",
    "\n",
    "    try:\n",
    "        # CSV file exists control\n",
    "        file_exists = os.path.exists(f\"data/{csv_file}\")\n",
    "\n",
    "        with open(f\"data/{csv_file}\", \"a\", newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            # Control that adds the header line if the file is written for the first time\n",
    "            if not file_exists:\n",
    "                writer.writerow([\"date\", \"title\", \"media_type\", \"url\"])\n",
    "\n",
    "            for entry in data:\n",
    "                # Checks required fields and writes to CSV file\n",
    "                writer.writerow([\n",
    "                    entry.get('date', ''),\n",
    "                    entry.get('title', '').replace('\\n', ' '),  # Cleans new lines in the title\n",
    "                    entry.get('media_type', ''),\n",
    "                    entry.get('url', '')\n",
    "                ])\n",
    "\n",
    "        print(f\"Data written to {csv_file} successfully.\")\n",
    "\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while writing the file: {e}\")\n",
    "\n"
   ],
   "id": "a66a8889152da9fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "write_apod_summary_to_csv()",
   "id": "cf296c21f9150fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem 3 - Numpy Array Manipulation and Statistical Functions (18 marks)\n",
    "### 3.1 NumPy Array Creation and Compliance with Terms - 2D NumPy Array Creation (7 marks)\n",
    "\n",
    "#### Aim\n",
    "\n",
    " • The sum of each row must be even.\n",
    " • The sum of all values in the array must be a multiple of 5.\n",
    "\n",
    "#### İşleyiş\n",
    "\n",
    "- First of all, I will create a NumPy array with random integers, then I will check the conditions and complete the task by changing the array elements if necessary."
   ],
   "id": "8922ecd66db5fd0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to create and condition a 2D NumPy array\n",
    "def create_array_with_conditions(rows=20, cols=5, min_val=10, max_val=100):\n",
    "    \"\"\"\n",
    "    Creates a 2D NumPy array containing random integers between 10 and 100 and satisfies certain conditions.\n",
    "     - The sum of each row must be even.\n",
    "      - The sum of all values in the array must be a multiple of 5.\n",
    "    \n",
    "    Args:\n",
    "    - rows (int): Number of rows. Defaults to 20.\n",
    "    - cols (int): Number of columns. Defaults to 5.\n",
    "    - min_val (int): Minimum integer value. Defaults to 10.\n",
    "    - max_val (int): Maximum integer value. Defaults to 100.\n",
    "    \n",
    "    Returns:\n",
    "    - np.ndarray: A NumPy array that satisfies the conditions.\n",
    "    \"\"\"\n",
    "    array = np.random.randint(min_val, max_val, (rows, cols))\n",
    "\n",
    "    # Loop that ensures the sum of each row is even\n",
    "    for i in range(rows):\n",
    "        if array[i].sum() % 2 != 0:  # check phase if sum is not even\n",
    "            array[i, 0] += 1  # Adjust the sum by incrementing the first element of the array by 1\n",
    "\n",
    "    # Make sure the sum of all values in the array is a multiple of 5\n",
    "    while array.sum() % 5 != 0:\n",
    "        array[0, 0] += 1  # Adjust the sum by incrementing the first element of the array by 1\n",
    "\n",
    "    return array"
   ],
   "id": "5ad28df09870f378",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a 2D NumPy array with conditions\n",
    "array_with_conditions = create_array_with_conditions()\n",
    "print(\"2D NumPy array with conditions:\")\n",
    "print(array_with_conditions)"
   ],
   "id": "33cd1eaef5a1654f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.2 NumPy Array Manipulation - Array Selection and Replacement (6 marks)\n",
    "\n",
    "#### Target:\n",
    "\n",
    "In the specified task, on the 2D NumPy array we created\n",
    "- We will select elements from the array that are divisible by both 3 and 5 and print them on the screen.\n",
    "- We will replace all elements in the array that are greater than 75 with the average of all elements of the array.\n",
    "\n",
    "#### Definitions:\n",
    "\n",
    "- We will select the elements in the array using `numpy`'s conditional operations.\n",
    "- We will use NumPy's indexing feature to manipulate elements.\n"
   ],
   "id": "2843f15da9803975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select elements divisible by 3 and 5\n",
    "divisible_by_3_and_5 = array_with_conditions[(array_with_conditions % 3 == 0) & (array_with_conditions % 5 == 0)]\n",
    "print(\"Elements divisible by 3 and 5:\")\n",
    "print(divisible_by_3_and_5)\n",
    "\n",
    "# Calculate the mean value of the array\n",
    "mean_value = array_with_conditions.mean()\n",
    "\n",
    "# Replace elements greater than 75 with the mean valu\n",
    "array_with_conditions[array_with_conditions > 75] = mean_value\n",
    "print(\"\\nElements greater than 75 are replaced with the average of the array:\")\n",
    "print(array_with_conditions)\n"
   ],
   "id": "a0c11d8780e743a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Perform some statistical operations on the array - Statistical Analysis (6 marks)\n",
    "#### Aim\n",
    "In this step, we will perform statistical analysis on the 2D NumPy array we created. We will perform the following actions:\n",
    "- We will calculate the average of all elements of the array.\n",
    "- We will calculate the standard deviation of all elements of the array.\n",
    "- We will find the median value of the series.\n",
    "- We will calculate the variance of each column.\n",
    "\n",
    "#### Definitions:\n",
    "\n",
    "This will perform these operations using the statistical functions of the NumPy library.\n"
   ],
   "id": "be3517e22dfa0b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mean of the array\n",
    "mean_value = np.mean(array_with_conditions)\n",
    "print(\"Mean of the array:\", mean_value)\n",
    "\n",
    "# Standart deviation of the array\n",
    "std_dev = np.std(array_with_conditions)\n",
    "print(\"Standart deviation of the array:\", std_dev)\n",
    "\n",
    "# Median value of the array\n",
    "median_value = np.median(array_with_conditions)\n",
    "print(\"Median value of the array:\", median_value)\n",
    "\n",
    "# Variance of each column\n",
    "column_variances = np.var(array_with_conditions, axis=0)\n",
    "print(\"Variance of each column:\")\n"
   ],
   "id": "78a4f611401f8f24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problem 4 - Working with Pandas DataFrames (22 marks)\n",
    "### 4.1 Basic Analysis of the Iris Data Set (4 marks)\n",
    "\n",
    "#### Aim\n",
    "\n",
    "- This stage involves reading the `iris.csv` file into Python as a `pandas` DataFrame and answering the following questions:\n",
    "* The data set has how many points?\n",
    "* What are column data types?\n",
    "* What are column names?\n",
    "* How many flower kinds are in the data set?\n",
    "\n",
    "\n",
    "#### Operation\n",
    "\n",
    "Basic analysis will be performed on the DataFrame after reading the CSV file using the `pandas` library.\n"
   ],
   "id": "236b91bfe8004a86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# iris.csv reading the file\n",
    "iris_df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# 1. The number of data points\n",
    "total_data_points = iris_df.shape[0]\n",
    "print(\"Total number of data points:\", total_data_points)\n",
    "\n",
    "# 2. Column data types\n",
    "print(\"\\nColumn data types:\")\n",
    "print(iris_df.dtypes)\n",
    "\n",
    "# 3. Column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(iris_df.columns)\n",
    "\n",
    "# 4. Number of flower kinds in the data set\n",
    "unique_species = iris_df['Species'].nunique()\n",
    "print(\"\\nNumber of flower kinds in the data set:\", unique_species)\n"
   ],
   "id": "e20698e719247748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2 Correcting Incorrect Data in the Iris Data Set (3 marks)\n",
    "\n",
    "#### Amaç\n",
    "\n",
    "As stated in the documentation, there are incorrect rows in the irish data set. We will fix these erroneous lines.\n",
    " The lines with errors are:\n",
    "- Line 35: Values will be corrected to `4.9, 3.1, 1.5, 0.2, \"setosa\"`.\n",
    "- Line 38: Values will be corrected to `4.9, 3.6, 1.4, 0.1, \"setosa\"`.\n",
    "\n",
    "#### Operation\n",
    "\n",
    "- Data will be read using `pandas` DataFrame.\n",
    "- Defective rows will be replaced with proper values.\n",
    "- Printing lines 35 and 38 to the screen will verify modifications.\n",
    "\n"
   ],
   "id": "ecbe7894f158dc8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# iris.csv reading the file\n",
    "iris_df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# Fix line 35 (based on 1-indexing, index 34 in Python)\n",
    "iris_df.loc[34] = [35, 4.9, 3.1, 1.5, 0.2, \"Iris-setosa\"]\n",
    "\n",
    "# Fix line 38 \n",
    "iris_df.loc[37] = [38, 4.9, 3.6, 1.4, 0.1, \"Iris-setosa\"]\n",
    "\n",
    "# Print lines 35 and 38 to confirm changes\n",
    "print(\"Corrected 35th row:\")\n",
    "print(iris_df.loc[34])\n",
    "\n",
    "print(\"\\nCorrected 38th row:\")\n",
    "print(iris_df.loc[37])\n"
   ],
   "id": "ceaf551df6e791bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.3 Adding New Features to the Iris Data Set (2 marks)\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "• Petal Ratio: Defined as the ratio of petal length to petal width.\n",
    "• Sepal Ratio: Defined as the ratio of sepal length to sepal width.\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "After adding these properties to the `pandas` DataFrame, we will save the updated DataFrame in the `iris_corrected.csv` file."
   ],
   "id": "d72dd8652c8a6fd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# iris.csv read\n",
    "iris_df = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# adad new features\n",
    "iris_df['Petal Ratio'] = iris_df['PetalLengthCm'] / iris_df['PetalWidthCm']\n",
    "iris_df['Sepal Ratio'] = iris_df['SepalLengthCm'] / iris_df['SepalWidthCm']\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "iris_df.to_csv(\"data/iris_corrected.csv\", index=False)\n",
    "\n",
    "print(\"Updated DataFrame saved to 'iris_corrected.csv'.\")\n"
   ],
   "id": "23bafec3d63d1ebf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.4 Calculating Correlation Between Numerical Features in the Iris Data Set (4 marks)\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "In this section, I will calculate the pairwise correlation between all numerical columns in the iris dataset\n",
    "- We will identify the two pairs of features that show the highest positive correlation.\n",
    "- We will identify the two feature pairs with the highest negative correlation.\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "- We will read the updated dataset using `pandas`."
   ],
   "id": "9b605be63faf64bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# iris_corrected.csv read\n",
    "iris_df = pd.read_csv(\"data/iris_corrected.csv\")\n",
    "\n",
    "# Selecting only numerical columns because we dont want to calculate correlation for Id column\n",
    "numeric_columns = iris_df.select_dtypes(include=['float64', 'int64']).drop(columns=['Id'])\n",
    "\n",
    "# Calculating the correlation matrix\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "# Printing the correlation matrix\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Selecting only the upper triangle of the correlation matrix to avoid repeating values\n",
    "# Stacking the correlation matrix to a multi-index series\n",
    "corr_unstacked = correlation_matrix.where(~correlation_matrix.isin([1.0])).stack()\n",
    "max_positive_corr = corr_unstacked.idxmax()\n",
    "min_negative_corr = corr_unstacked.idxmin()\n",
    "\n",
    "print(\"\\nHighest positive correlation:\", max_positive_corr, correlation_matrix.loc[max_positive_corr])\n",
    "print(\"\\nHighest negative correlation:\", min_negative_corr, correlation_matrix.loc[min_negative_corr])\n"
   ],
   "id": "2720629ac163e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.5 Creating a Scatter Plot with Regression Lines (5 marks)\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "In this step:\n",
    "- We will create a scatter plot by taking **Sepal Ratio** to the x axis and **Petal Ratio** to the y axis.\n",
    "- We will show the dots with different colors according to the species.\n",
    "- We will add a linear regression line for each species.\n",
    "- We will save the chart in the `iris_scatter_with_regression.pdf` file.\n",
    "\n",
    "#### Definitions\n",
    "- We will create graphics using `seaborn` and `matplotlib` libraries and save them in pdf format.\n"
   ],
   "id": "611afd341f1c2482"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns  # !pip install seaborn\n",
    "import matplotlib.pyplot as plt  # !pip install matplotlib\n",
    "from sklearn.linear_model import LinearRegression  # !pip install scikit-learn\n",
    "import numpy as np\n",
    "\n",
    "# iris_corrected.csv read\n",
    "iris_df = pd.read_csv(\"data/iris_corrected.csv\")\n",
    "\n",
    "# Scatter plot creation\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=iris_df, x='Sepal Ratio', y='Petal Ratio', hue='Species', palette='viridis')\n",
    "\n",
    "# Linear regression for each species\n",
    "species_list = iris_df['Species'].unique()\n",
    "for species in species_list:\n",
    "    subset = iris_df[iris_df['Species'] == species]\n",
    "    X = subset['Sepal Ratio'].values.reshape(-1, 1)\n",
    "    y = subset['Petal Ratio'].values\n",
    "\n",
    "    # Linear regression model fitting\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Plotting the regression line\n",
    "    plt.plot(X, y_pred, label=f\"{species} Regression Line\", linewidth=2)\n",
    "\n",
    "# Adding labels and legend\n",
    "plt.title(\"Sepal Ratio & Petal Ratio Scatter Plot\")\n",
    "plt.xlabel(\"Sepal Ratio\")\n",
    "plt.ylabel(\"Petal Ratio\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Saving the plot as a pdf fşile\n",
    "plt.savefig(\"data/iris_scatter_with_regression.pdf\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Graph saved as 'iris_scatter_with_regression.pdf'\")\n"
   ],
   "id": "7f85fdc966a359c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.6 Creating a Pair Plot with New Features (4 marks)\n",
    "\n",
    "#### Purpose:\n",
    "- We will create a pair plot for the four original numerical features in the iris dataset (`SepalLengthCm`, `SepalWidthCm`, `PetalLengthCm`, `PetalWidthCm`) and the two new features we added (`Petal Ratio`, `Sepal Ratio`).\n",
    "- We will color the dots according to the species.\n",
    "\n",
    "#### Definitions:\n",
    "\n",
    "- We will use the `seaborn` library to create the pair plot."
   ],
   "id": "6b948c5b23f40ffe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris_df = pd.read_csv(\"data/iris_corrected.csv\")\n",
    "\n",
    "# Adding new features\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.pairplot(iris_df, hue='Species',\n",
    "             vars=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Petal Ratio', 'Sepal Ratio'],\n",
    "             palette='viridis')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Pair plot created and shown\")\n"
   ],
   "id": "6b8b2aa40287b81e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
